{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import sys,json\n",
    "sys.path.append('./graphparser')\n",
    "import graphparser as gp\n",
    "reload(gp)\n",
    "from networkx.readwrite import json_graph\n",
    "urdu_data_file = './graphparser/settings/urdu.yaml'\n",
    "devanagari_data_file = './graphparser/settings/devanagari.yaml'\n",
    "diacritics_data_file = './graphparser/settings/diacritics.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = gp.GraphParser(urdu_data_file)\n",
    "assert parser.parse(\"shaan\").output==u'\\u0634\\u0627\\u0646'\n",
    "assert parser.parse('karegaa').output==u'\\u06a9\\u0631\\u06d2\\u06af\\u0627'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserOutput(matches=[ParserRule(production=u'\\u092e', prev_classes=None, prev_tokens=None, tokens=['m'], next_tokens=None, next_classes=None, prev_length=0, next_length=1, match_tokens=['m'], tokens_length=1), ParserRule(production=u'\\u0941', prev_classes=None, prev_tokens=None, tokens=['u'], next_tokens=None, next_classes=None, prev_length=0, next_length=1, match_tokens=['u'], tokens_length=1), ParserRule(production=u'\\u0926', prev_classes=None, prev_tokens=None, tokens=['d'], next_tokens=None, next_classes=None, prev_length=0, next_length=1, match_tokens=['d'], tokens_length=1), ParserRule(production=u'\\u0926', prev_classes=None, prev_tokens=None, tokens=['d'], next_tokens=None, next_classes=None, prev_length=0, next_length=1, match_tokens=['d'], tokens_length=1), ParserRule(production=u'\\u0906', prev_classes=['consonant'], prev_tokens=None, tokens=['a', '((', 'aa'], next_tokens=None, next_classes=None, prev_length=0, next_length=3, match_tokens=['a', '((', 'aa'], tokens_length=3)], output=u'\\u092e\\u0941\\u0926\\u094d\\u094d\\u0926\\u0906')\n"
     ]
    }
   ],
   "source": [
    "dev_parser = gp.GraphParser(devanagari_data_file)\n",
    "print dev_parser.parse('mudda((aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compress_nodes(parser):\n",
    "#    ''' CSub'''\n",
    "    dg = parser.DG.copy()\n",
    "\n",
    "    def classes_int(classes):\n",
    "        return [parser.token_class_names.index(c) for c in classes]\n",
    "\n",
    "    \n",
    "\n",
    "    for n,d in dg.nodes(data=True):\n",
    "        if 'found' in d:\n",
    "            d.pop('found') # not needed\n",
    "        if 'rule' in d:\n",
    "            r = d['rule']\n",
    "            d.pop('rule') # already contained on edge, but signals end of road\n",
    "            d['r'] ={'p':r.production,'tl':r.tokens_length}\n",
    "        if 'token' in d:\n",
    "            d['t'] = d['token']\n",
    "            d.pop('token')\n",
    "            \n",
    "    return dg.node\n",
    "\n",
    "def compress_edges(parser):\n",
    "    \n",
    "    def classes_int(classes):\n",
    "        return [parser.token_class_names.index(c) for c in classes]\n",
    "\n",
    "\n",
    "    def shorten_parser_rule(pr):\n",
    "        o={}\n",
    "\n",
    "        if pr.prev_classes:\n",
    "            o['pc'] = classes_int(pr.prev_classes)\n",
    "#        if pr.prev_tokens:\n",
    "#            o['pt'] = classes_int(pr.prev_tokens)\n",
    "        o['mt'] = pr.match_tokens \n",
    "#        o['t'] = pr.tokens\n",
    "\n",
    " #       if pr.next_tokens:\n",
    " #           o['nt'] = classes_int(pr.next_tokens)\n",
    "        o['pl'] = pr.prev_length\n",
    "        o['nl'] = pr.next_length\n",
    "        if pr.next_classes:\n",
    "            o['nc'] = classes_int(pr.next_classes)\n",
    "        o['tl'] = pr.tokens_length\n",
    "        print pr.tokens_length\n",
    "        o['p'] = pr.production\n",
    "\n",
    "        return o\n",
    "    dg = parser.DG.copy()\n",
    "\n",
    "    x = dict(parser.get_sorted_out_edges(dg))\n",
    "\n",
    "    for node_start, values in x.iteritems():\n",
    "        for u,v,d in values:\n",
    "            if 'weight' in d: d.pop('weight')\n",
    "            if 'rule' in d:\n",
    "                d['r'] = shorten_parser_rule(d['rule'])\n",
    "                d.pop('rule')\n",
    "        \n",
    "    \n",
    "    return x\n",
    "\n",
    "parser = gp.GraphParser(urdu_data_file)\n",
    "#compress_edges(parser)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compress_onmatch_rules(parser):\n",
    "    omr = parser.onmatch_rules\n",
    "    if omr==None: return omr\n",
    "    x = list()\n",
    "    for ((prev_class, next_class), output) in omr:\n",
    "        x.append( ( ( parser.classes_int(prev_class), parser.classes_int(next_class) ), output) )\n",
    "    return x\n",
    "\n",
    "#for x in parser.onmatch_rules\n",
    "\n",
    "def compress_tokens(parser):\n",
    "    \n",
    "    def classes_int(classes):\n",
    "        return [parser.token_class_names.index(c) for c in classes]\n",
    "    output ={}\n",
    "    for token,classes in parser.tokens.iteritems():\n",
    "        output[token]=classes_int(classes)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def gen_parser_data(parser_name = 'urdu',\n",
    "                    parser_data_file = './graphparser/settings/urdu.yaml',\n",
    "                    output_file = 'lib/urdu_parser_data.js',\n",
    "                    compress=True):\n",
    "    parser = gp.GraphParser(parser_data_file)\n",
    "    assert parser\n",
    "    token_match_re_string=parser.get_token_match_re_string()\n",
    "    \n",
    "    graph = parser.DG\n",
    "\n",
    "    compress=True\n",
    "    \n",
    "    if compress:\n",
    "        nodes = compress_nodes(parser)\n",
    "        sorted_edges = compress_edges(parser)\n",
    "    else:\n",
    "        nodes = parser.DG.node\n",
    "        sorted_edges = parser.get_sorted_out_edges(parser.DG)\n",
    "        \n",
    "\n",
    "    tokens = parser.tokens\n",
    "   \n",
    "    if compress:\n",
    "        tokens = compress_tokens(parser)\n",
    "# sorted by weight, reversed\n",
    "    \n",
    "    \n",
    "\n",
    "    onmatch = parser.onmatch_rules\n",
    "    \n",
    "    if onmatch!=None and compress:\n",
    "\n",
    "        onmatch = compress_onmatch_rules(parser)\n",
    "    \n",
    "    onmatch_rules_json = json.dumps(onmatch ,separators=(',', ':') ) #skinny_onmatch_rules(parser))\n",
    "    \n",
    "     \n",
    "\n",
    "   \n",
    "    \n",
    "    graph_json = json.dumps( {'node': nodes, 'edge': sorted_edges, 'compressed': compress} ,separators=(',', ':') )\n",
    "    \n",
    "    js_template = \"\"\"\n",
    "{PARSER_NAME}_tokens = {TOKENS};\n",
    "token_match_re_string = {TOKEN_MATCH_RE_STRING};\n",
    "{PARSER_NAME}_token_regex = new RegExp(token_match_re_string, 'g');\n",
    "graph_json = {GRAPH_JSON};\n",
    "onmatch_json = {ONMATCH_RULES_JSON};\n",
    "\n",
    "function decode_json(x){{return x}};\n",
    "\n",
    "{PARSER_NAME}_graph = decode_json(graph_json);\n",
    "{PARSER_NAME}_onmatch = decode_json(onmatch_json);\n",
    "\n",
    "console.log('parser loaded; len of graph: '+graph_json);\n",
    "\"\"\"\n",
    "    \n",
    "    js_output = js_template.format(PARSER_NAME = parser_name,\n",
    "           TOKENS = json.dumps( tokens ),\n",
    "           TOKEN_MATCH_RE_STRING = json.dumps(token_match_re_string),\n",
    "           GRAPH_JSON = graph_json,\n",
    "           ONMATCH_RULES_JSON = onmatch_rules_json)\n",
    "    \n",
    "    with open(output_file,'w') as f:\n",
    "        f.write(js_output)\n",
    "\n",
    "gen_parser_data()\n",
    "\n",
    "gen_parser_data(parser_name = 'devanagari',\n",
    "                parser_data_file = './graphparser/settings/devanagari.yaml',\n",
    "                output_file = 'lib/devanagari_parser_data.js',\n",
    "                compress=True)\n",
    "\n",
    "gen_parser_data(parser_name = 'diacritics',\n",
    "                parser_data_file = './graphparser/settings/diacritics.yaml',\n",
    "                output_file = 'lib/diacritics_parser_data.js',\n",
    "                compress=True)\n",
    "\n",
    "#!coffee --compile --bare --output lib/ src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
